distribution = "bernoulli",
n.trees = 5000,
interaction.depth = 4,
shrinkage = 0.1)
y_hat <- predict(gradientBoost, x_test)
confusion_matrix <- confusionMatrix(data = as.factor(y_hat), reference = as.factor(test_set$response))
p = 30
x_train <- read.csv("cancer.train.csv", header = FALSE)
y_train <- read.csv("label.train.csv", header = FALSE)
colnames(y_train) <- c("response")
y_train$response <- as.factor(y_train$response)
train_set <- cbind(x_train, y_train)
x_test <- read.csv("cancer.test.csv", header = FALSE)
y_test <- read.csv("label.test.csv", header = FALSE)
colnames(y_test) <- c("response")
y_test$response <- as.factor(y_test$response)
test_set <- cbind(x_test, y_test)
train_set$response <- as.numeric(train_set$response)
gradientBoost <- gbm(response ~ ., data = train_set,
distribution = "bernoulli",
n.trees = 5000,
interaction.depth = 4,
shrinkage = 0.1)
y_hat <- predict(gradientBoost, x_test)
confusion_matrix <- confusionMatrix(data = as.factor(y_hat), reference = as.factor(test_set$response))
View(y_test)
View(y_test)
p = 30
x_train <- read.csv("cancer.train.csv", header = FALSE)
y_train <- read.csv("label.train.csv", header = FALSE)
colnames(y_train) <- c("response")
as.factor(mapvalues(bc$Class, c("benign", "malignant"), c("0","1")))
require(ISLR2)
require(tree)
require(randomForest)
require(gbm)
require(caret)
require(plyr)
p = 30
x_train <- read.csv("cancer.train.csv", header = FALSE)
y_train <- read.csv("label.train.csv", header = FALSE)
colnames(y_train) <- c("response")
as.factor(mapvalues(bc$Class, c("benign", "malignant"), c("0","1")))
p = 30
x_train <- read.csv("cancer.train.csv", header = FALSE)
y_train <- read.csv("label.train.csv", header = FALSE)
colnames(y_train) <- c("response")
y_train$response <- as.factor(mapvalues(y_train$response, c("B", "M"), c("0","1")))
train_set <- cbind(x_train, y_train)
x_test <- read.csv("cancer.test.csv", header = FALSE)
y_test <- read.csv("label.test.csv", header = FALSE)
colnames(y_test) <- c("response")
y_test$response <- as.factor(mapvalues(y_test$response, c("B", "M"), c("0","1")))
test_set <- cbind(x_test, y_test)
View(y_test)
rforest <- randomForest(response~., data = train_set,
mtry = p,
ntree = 500)
y_hat <- predict(rforest, x_test)
confusion_matrix <- confusionMatrix(data = y_hat, reference = y_test$response)
paste0("Error: ", 1 - as.numeric(confusion_matrix$overall["Accuracy"][1]))
confusion_matrix$table
rforest <- randomForest(response~., data = train_set,
mtry = as.integer(p/2),
ntree = 500)
y_hat <- predict(rforest, x_test)
confusion_matrix <- confusionMatrix(data = y_hat, reference = y_test$response)
paste0("Error: ", 1 - as.numeric(confusion_matrix$overall["Accuracy"][1]))
confusion_matrix$table
rforest <- randomForest(response~., data = train_set,
mtry = as.integer(sqrt(p)),
ntree = 500)
y_hat <- predict(rforest, x_test)
confusion_matrix <- confusionMatrix(data = y_hat, reference = y_test$response)
paste0("Error: ", 1 - as.numeric(confusion_matrix$overall["Accuracy"][1]))
confusion_matrix$table
rforest <- randomForest(response~., data = train_set,
mtry = as.integer(sqrt(p)),
ntree = 100)
y_hat <- predict(rforest, x_test)
confusion_matrix <- confusionMatrix(data = y_hat, reference = y_test$response)
paste0("Error: ", 1 - as.numeric(confusion_matrix$overall["Accuracy"][1]))
confusion_matrix$table
gradientBoost <- gbm(response ~ ., data = train_set,
distribution = "bernoulli",
n.trees = 5000,
interaction.depth = 4,
shrinkage = 0.1)
y_hat <- predict(gradientBoost, x_test)
#confusion_matrix <- confusionMatrix(data = as.factor(y_hat), reference = as.factor(test_set$response))
train_set$response <- as.character(train_set$response)
gradientBoost <- gbm(response ~ ., data = train_set,
distribution = "bernoulli",
n.trees = 5000,
interaction.depth = 4,
shrinkage = 0.1)
y_hat <- predict(gradientBoost, x_test)
#confusion_matrix <- confusionMatrix(data = as.factor(y_hat), reference = as.factor(test_set$response))
p = 30
x_train <- read.csv("cancer.train.csv", header = FALSE)
y_train <- read.csv("label.train.csv", header = FALSE)
colnames(y_train) <- c("response")
y_train$response <- as.factor(mapvalues(y_train$response, c("B", "M"), c("0","1")))
train_set <- cbind(x_train, y_train)
x_test <- read.csv("cancer.test.csv", header = FALSE)
y_test <- read.csv("label.test.csv", header = FALSE)
colnames(y_test) <- c("response")
y_test$response <- as.factor(mapvalues(y_test$response, c("B", "M"), c("0","1")))
test_set <- cbind(x_test, y_test)
rforest <- randomForest(response~., data = train_set,
mtry = p,
ntree = 500)
y_hat <- predict(rforest, x_test)
confusion_matrix <- confusionMatrix(data = y_hat, reference = y_test$response)
paste0("Error: ", 1 - as.numeric(confusion_matrix$overall["Accuracy"][1]))
confusion_matrix$table
gradientBoost <- gbm(response ~ ., data = train_set,
distribution = "poisson",
n.trees = 5000,
interaction.depth = 4,
shrinkage = 0.1)
gradientBoost <- gbm(response ~ ., data = train_set,
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 4,
shrinkage = 0.1)
y_hat <- predict(gradientBoost, x_test)
#confusion_matrix <- confusionMatrix(data = as.factor(y_hat), reference = as.factor(test_set$response))
View(y_test)
y_hat
gradientBoost <- gbm(response ~ ., data = train_set,
distribution = "bernoulli",
n.trees = 5000,
interaction.depth = 4,
shrinkage = 0.1)
y_hat <- predict(gradientBoost, x_test)
#confusion_matrix <- confusionMatrix(data = as.factor(y_hat), reference = as.factor(test_set$response))
setwd("C:/Users/nakul/Documents/StudentSuccess")
require(xgboost)
require(Matrix)
require(data.table)
require(caret)
require(mltools)
require(data.table)
require(progress)
df <- read.csv("data/por_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
Y
df$Student_Class
df <- read.csv("data/math_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
df$Student_Class
data_math = read.table("student/student-mat.csv",sep=";",header=TRUE)
data_math = read.table("data/student-mat.csv",sep=";",header=TRUE)
data_por = read.table("data/student-por.csv",sep=";",header=TRUE)
data_all = merge(data_math,data_por,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"))
data_math = read.table("data/student-mat.csv",sep=";",header=TRUE)
data_por = read.table("data/student-por.csv",sep=";",header=TRUE)
data_all = merge(data_math,data_por,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"))
#Function: get_score
#Description:
#This function takes the students grades from the 3 trimesters and generates a
#score that will be used to create classes for the classification algorithms
#inputs:
#->data: data frame of student performance
#outputs:
#->student_score: vector of score values for student performance
get_score = function(data){
student_score = NULL
for(i in 1:dim(data)[1]){
grades = as.numeric(unlist(data[i, 31:33]))
d_avg = mean(grades[3] - grades[2], grades[2] - grades[1])
student_score = c(student_score, (1 + (d_avg / 20))*data[i,33])
}
return(student_score)
}
#Generate Student Scores and add to data frames
student_score_math = get_score(data_math)
student_score_por = get_score(data_por)
data_math = cbind(data_math, Student_Score = student_score_math)
data_por = cbind(data_por, Student_Score = student_score_por)
#Plot of histograms of both student score vectors
par(mfrow = c(2,1))
hist(student_score_math, main = "Student Scores for Math", xlab = "Student Score")
hist(student_score_por,main = "Student Scores for Portugese", xlab = "Student Score")
#Create classes
#Function: create_classes
#Description:
#This function takes the the student score values and determines the boundaries of each class
#inputs:
#->student score: vector of score values for student performance
#outputs:
#->bounds: vector containing two boundaries of the classes
create_classes = function(student_score){
size = ceiling(length(student_score) / 3)
temp = sort(student_score)
bound1 = temp[size]
bound2 = temp[2 * size]
return(c(bound1, bound2))
}
#Create boundaries of classes
bounds_math = create_classes(student_score_math)
bounds_por = create_classes(student_score_por)
#Assign Classes
#Function: assign_class
#Description:
#This function uses the bounds and the score values to assign classes
#inputs:
#->student score: vector of score values for student performance
#->bounds: vector containing two boundaries of the classes
#outputs:
#->student_class: vector of class assignments
assign_class = function(student_score, bounds){
student_class = NULL
#Loop through scores
for(i in 1:length(student_score)){
#Select apropriate class based on score
if(student_score[i] < bounds[1]){
student_class = c(student_class, 1)
}
else if(student_score[i] >= bounds[2]){
student_class = c(student_class, 3)
}
else{
student_class = c(student_class, 2)
}
}
return(student_class)
}
#Create classes and append to data frame
classes_math = assign_class(student_score_math, bounds_math)
classes_por = assign_class(student_score_por, bounds_por)
data_math = cbind(data_math, Student_Class = as.factor(classes_math))
data_por = cbind(data_por, Student_Class = as.factor(classes_por))
View(data_por)
write.csv(data_math, file = "data/math_data_cleaned.csv")
df <- read.csv("data/math_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
View(df)
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
Y
partition = createDataPartition(
Y,
times = 1,
p = 0.7,
list = TRUE,
groups = min(5, length(Y))
)
X_train = X[partition$Resample1,]
X_test = X[-partition$Resample1,]
Y_train = Y[partition$Resample1]
Y_test = Y[-partition$Resample1]
max_depth_vals <- seq(3,10,1)
min_child_weight_vals <- seq(1,6,1)
gamma_vals <- seq(0,.5,.1)
results <- data.frame(max_depth = c(),
min_child_weight = c(),
gamma = c(),
merror = c())
total <- length(max_depth_vals) * length(min_child_weight_vals) * length(gamma_vals)
pb <- progress_bar$new(
format = "  Running [:bar] :percent . Finished in :elapsed",
total = total, clear = FALSE, width= 100)
pb$tick(0)
for (max_depth in max_depth_vals){
for (min_child_weight in min_child_weight_vals){
for(gamma in gamma_vals){
cv <- xgb.cv(data = as.matrix(X),
label = Y,
objective = "multi:softmax",
verbose = 0,
num_class = 3,
nfold = 5,
metrics = list("merror"),
nrounds = 70,
eta = 0.01,
max_depth = max_depth,
min_child_weight = min_child_weight,
gamma = gamma,
early_stopping_rounds = 25)
res <- list(max_depth = max_depth,
min_child_weight = min_child_weight,
gamma = gamma,
merror = min(cv$evaluation_log$train_merror_mean))
results <- rbind(results, res)
pb$tick()
}
}
}
max_depth_vals <- seq(3,10,1)
min_child_weight_vals <- seq(1,6,1)
gamma_vals <- seq(0,.5,.1)
results <- data.frame(max_depth = c(),
min_child_weight = c(),
gamma = c(),
merror = c())
total <- length(max_depth_vals) * length(min_child_weight_vals) * length(gamma_vals)
pb <- progress_bar$new(
format = "  Running [:bar] :percent . Finished in :elapsed",
total = total, clear = FALSE, width= 100)
pb$tick(0)
for (max_depth in max_depth_vals){
for (min_child_weight in min_child_weight_vals){
for(gamma in gamma_vals){
cv <- xgb.cv(data = as.matrix(X),
label = Y,
objective = "multi:softmax",
verbose = 0,
num_class = 3,
nfold = 5,
metrics = list("merror"),
nrounds = 70,
eta = 0.01,
max_depth = max_depth,
min_child_weight = min_child_weight,
gamma = gamma,
early_stopping_rounds = 25)
res <- list(max_depth = max_depth,
min_child_weight = min_child_weight,
gamma = gamma,
merror = min(cv$evaluation_log$train_merror_mean))
results <- rbind(results, res)
pb$tick()
}
}
}
bstDense <- xgboost(data = as.matrix(X_train),
label= Y_train,
objective = "multi:softmax",
num_class = 3,
verbose = 0,
nrounds = 70,
eta = 0.01,
max_depth = 10,
min_child_weight = 1,
gamma = 0.1
)
Y_hat <- predict(bstDense, as.matrix(X_test))
c_matrix <- confusionMatrix(data=as.factor(Y_hat), reference = as.factor(Y_test))
c_matrix
write.to_csv(data_por, file = "por_data_cleaned.csv")
write.csv(data_por, file = "data/por_data_cleaned.csv")
df <- read.csv("data/por_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
Y
partition = createDataPartition(
Y,
times = 1,
p = 0.7,
list = TRUE,
groups = min(5, length(Y))
)
X_train = X[partition$Resample1,]
X_test = X[-partition$Resample1,]
Y_train = Y[partition$Resample1]
Y_test = Y[-partition$Resample1]
bstDense <- xgboost(data = as.matrix(X_train),
label= Y_train,
objective = "multi:softmax",
num_class = 3,
verbose = 0,
nrounds = 70,
eta = 0.01,
max_depth = 10,
min_child_weight = 1,
gamma = 0.1
)
Y_hat <- predict(bstDense, as.matrix(X_test))
c_matrix <- confusionMatrix(data=as.factor(Y_hat), reference = as.factor(Y_test))
c_matrix
importance <- xgb.importance(model = bstDense)
xgb.plot.importance(importance[1:10,])
importance <- xgb.importance(model = bstDense)
xgb.plot.importance(importance[1:10,])
importance <- xgb.importance(model = bstDense)
xgb.plot.importance(importance[1:5,])
df <- read.csv("data/math_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
df <- read.csv("data/math_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
partition = createDataPartition(
Y,
times = 1,
p = 0.7,
list = TRUE,
groups = min(5, length(Y))
)
X_train = X[partition$Resample1,]
X_test = X[-partition$Resample1,]
Y_train = Y[partition$Resample1]
Y_test = Y[-partition$Resample1]
bstDense <- xgboost(data = as.matrix(X_train),
label= Y_train,
objective = "multi:softmax",
num_class = 3,
verbose = 0,
nrounds = 70,
eta = 0.01,
max_depth = 10,
min_child_weight = 1,
gamma = 0.1
)
Y_hat <- predict(bstDense, as.matrix(X_test))
c_matrix <- confusionMatrix(data=as.factor(Y_hat), reference = as.factor(Y_test))
c_matrix
df <- read.csv("data/por_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
partition = createDataPartition(
Y,
times = 1,
p = 0.7,
list = TRUE,
groups = min(5, length(Y))
)
X_train = X[partition$Resample1,]
X_test = X[-partition$Resample1,]
Y_train = Y[partition$Resample1]
Y_test = Y[-partition$Resample1]
bstDense <- xgboost(data = as.matrix(X_train),
label= Y_train,
objective = "multi:softmax",
num_class = 3,
verbose = 0,
nrounds = 70,
eta = 0.01,
max_depth = 10,
min_child_weight = 1,
gamma = 0.1
)
Y_hat <- predict(bstDense, as.matrix(X_test))
c_matrix <- confusionMatrix(data=as.factor(Y_hat), reference = as.factor(Y_test))
c_matrix
importance <- xgb.importance(model = bstDense)
xgb.plot.importance(importance[1:5,])
importance <- xgb.importance(model = bstDense)
xgb.plot.importance(importance[1:5,])
