drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
df$Student_Class
data_math = read.table("student/student-mat.csv",sep=";",header=TRUE)
data_math = read.table("data/student-mat.csv",sep=";",header=TRUE)
data_por = read.table("data/student-por.csv",sep=";",header=TRUE)
data_all = merge(data_math,data_por,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"))
data_math = read.table("data/student-mat.csv",sep=";",header=TRUE)
data_por = read.table("data/student-por.csv",sep=";",header=TRUE)
data_all = merge(data_math,data_por,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"))
#Function: get_score
#Description:
#This function takes the students grades from the 3 trimesters and generates a
#score that will be used to create classes for the classification algorithms
#inputs:
#->data: data frame of student performance
#outputs:
#->student_score: vector of score values for student performance
get_score = function(data){
student_score = NULL
for(i in 1:dim(data)[1]){
grades = as.numeric(unlist(data[i, 31:33]))
d_avg = mean(grades[3] - grades[2], grades[2] - grades[1])
student_score = c(student_score, (1 + (d_avg / 20))*data[i,33])
}
return(student_score)
}
#Generate Student Scores and add to data frames
student_score_math = get_score(data_math)
student_score_por = get_score(data_por)
data_math = cbind(data_math, Student_Score = student_score_math)
data_por = cbind(data_por, Student_Score = student_score_por)
#Plot of histograms of both student score vectors
par(mfrow = c(2,1))
hist(student_score_math, main = "Student Scores for Math", xlab = "Student Score")
hist(student_score_por,main = "Student Scores for Portugese", xlab = "Student Score")
#Create classes
#Function: create_classes
#Description:
#This function takes the the student score values and determines the boundaries of each class
#inputs:
#->student score: vector of score values for student performance
#outputs:
#->bounds: vector containing two boundaries of the classes
create_classes = function(student_score){
size = ceiling(length(student_score) / 3)
temp = sort(student_score)
bound1 = temp[size]
bound2 = temp[2 * size]
return(c(bound1, bound2))
}
#Create boundaries of classes
bounds_math = create_classes(student_score_math)
bounds_por = create_classes(student_score_por)
#Assign Classes
#Function: assign_class
#Description:
#This function uses the bounds and the score values to assign classes
#inputs:
#->student score: vector of score values for student performance
#->bounds: vector containing two boundaries of the classes
#outputs:
#->student_class: vector of class assignments
assign_class = function(student_score, bounds){
student_class = NULL
#Loop through scores
for(i in 1:length(student_score)){
#Select apropriate class based on score
if(student_score[i] < bounds[1]){
student_class = c(student_class, 1)
}
else if(student_score[i] >= bounds[2]){
student_class = c(student_class, 3)
}
else{
student_class = c(student_class, 2)
}
}
return(student_class)
}
#Create classes and append to data frame
classes_math = assign_class(student_score_math, bounds_math)
classes_por = assign_class(student_score_por, bounds_por)
data_math = cbind(data_math, Student_Class = as.factor(classes_math))
data_por = cbind(data_por, Student_Class = as.factor(classes_por))
View(data_por)
write.csv(data_math, file = "data/math_data_cleaned.csv")
df <- read.csv("data/math_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
View(df)
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
Y
partition = createDataPartition(
Y,
times = 1,
p = 0.7,
list = TRUE,
groups = min(5, length(Y))
)
X_train = X[partition$Resample1,]
X_test = X[-partition$Resample1,]
Y_train = Y[partition$Resample1]
Y_test = Y[-partition$Resample1]
max_depth_vals <- seq(3,10,1)
min_child_weight_vals <- seq(1,6,1)
gamma_vals <- seq(0,.5,.1)
results <- data.frame(max_depth = c(),
min_child_weight = c(),
gamma = c(),
merror = c())
total <- length(max_depth_vals) * length(min_child_weight_vals) * length(gamma_vals)
pb <- progress_bar$new(
format = "  Running [:bar] :percent . Finished in :elapsed",
total = total, clear = FALSE, width= 100)
pb$tick(0)
for (max_depth in max_depth_vals){
for (min_child_weight in min_child_weight_vals){
for(gamma in gamma_vals){
cv <- xgb.cv(data = as.matrix(X),
label = Y,
objective = "multi:softmax",
verbose = 0,
num_class = 3,
nfold = 5,
metrics = list("merror"),
nrounds = 70,
eta = 0.01,
max_depth = max_depth,
min_child_weight = min_child_weight,
gamma = gamma,
early_stopping_rounds = 25)
res <- list(max_depth = max_depth,
min_child_weight = min_child_weight,
gamma = gamma,
merror = min(cv$evaluation_log$train_merror_mean))
results <- rbind(results, res)
pb$tick()
}
}
}
max_depth_vals <- seq(3,10,1)
min_child_weight_vals <- seq(1,6,1)
gamma_vals <- seq(0,.5,.1)
results <- data.frame(max_depth = c(),
min_child_weight = c(),
gamma = c(),
merror = c())
total <- length(max_depth_vals) * length(min_child_weight_vals) * length(gamma_vals)
pb <- progress_bar$new(
format = "  Running [:bar] :percent . Finished in :elapsed",
total = total, clear = FALSE, width= 100)
pb$tick(0)
for (max_depth in max_depth_vals){
for (min_child_weight in min_child_weight_vals){
for(gamma in gamma_vals){
cv <- xgb.cv(data = as.matrix(X),
label = Y,
objective = "multi:softmax",
verbose = 0,
num_class = 3,
nfold = 5,
metrics = list("merror"),
nrounds = 70,
eta = 0.01,
max_depth = max_depth,
min_child_weight = min_child_weight,
gamma = gamma,
early_stopping_rounds = 25)
res <- list(max_depth = max_depth,
min_child_weight = min_child_weight,
gamma = gamma,
merror = min(cv$evaluation_log$train_merror_mean))
results <- rbind(results, res)
pb$tick()
}
}
}
bstDense <- xgboost(data = as.matrix(X_train),
label= Y_train,
objective = "multi:softmax",
num_class = 3,
verbose = 0,
nrounds = 70,
eta = 0.01,
max_depth = 10,
min_child_weight = 1,
gamma = 0.1
)
Y_hat <- predict(bstDense, as.matrix(X_test))
c_matrix <- confusionMatrix(data=as.factor(Y_hat), reference = as.factor(Y_test))
c_matrix
write.to_csv(data_por, file = "por_data_cleaned.csv")
write.csv(data_por, file = "data/por_data_cleaned.csv")
df <- read.csv("data/por_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
Y
partition = createDataPartition(
Y,
times = 1,
p = 0.7,
list = TRUE,
groups = min(5, length(Y))
)
X_train = X[partition$Resample1,]
X_test = X[-partition$Resample1,]
Y_train = Y[partition$Resample1]
Y_test = Y[-partition$Resample1]
bstDense <- xgboost(data = as.matrix(X_train),
label= Y_train,
objective = "multi:softmax",
num_class = 3,
verbose = 0,
nrounds = 70,
eta = 0.01,
max_depth = 10,
min_child_weight = 1,
gamma = 0.1
)
Y_hat <- predict(bstDense, as.matrix(X_test))
c_matrix <- confusionMatrix(data=as.factor(Y_hat), reference = as.factor(Y_test))
c_matrix
importance <- xgb.importance(model = bstDense)
xgb.plot.importance(importance[1:10,])
importance <- xgb.importance(model = bstDense)
xgb.plot.importance(importance[1:10,])
importance <- xgb.importance(model = bstDense)
xgb.plot.importance(importance[1:5,])
df <- read.csv("data/math_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
df <- read.csv("data/math_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
partition = createDataPartition(
Y,
times = 1,
p = 0.7,
list = TRUE,
groups = min(5, length(Y))
)
X_train = X[partition$Resample1,]
X_test = X[-partition$Resample1,]
Y_train = Y[partition$Resample1]
Y_test = Y[-partition$Resample1]
bstDense <- xgboost(data = as.matrix(X_train),
label= Y_train,
objective = "multi:softmax",
num_class = 3,
verbose = 0,
nrounds = 70,
eta = 0.01,
max_depth = 10,
min_child_weight = 1,
gamma = 0.1
)
Y_hat <- predict(bstDense, as.matrix(X_test))
c_matrix <- confusionMatrix(data=as.factor(Y_hat), reference = as.factor(Y_test))
c_matrix
df <- read.csv("data/por_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
partition = createDataPartition(
Y,
times = 1,
p = 0.7,
list = TRUE,
groups = min(5, length(Y))
)
X_train = X[partition$Resample1,]
X_test = X[-partition$Resample1,]
Y_train = Y[partition$Resample1]
Y_test = Y[-partition$Resample1]
bstDense <- xgboost(data = as.matrix(X_train),
label= Y_train,
objective = "multi:softmax",
num_class = 3,
verbose = 0,
nrounds = 70,
eta = 0.01,
max_depth = 10,
min_child_weight = 1,
gamma = 0.1
)
Y_hat <- predict(bstDense, as.matrix(X_test))
c_matrix <- confusionMatrix(data=as.factor(Y_hat), reference = as.factor(Y_test))
c_matrix
importance <- xgb.importance(model = bstDense)
xgb.plot.importance(importance[1:5,])
importance <- xgb.importance(model = bstDense)
xgb.plot.importance(importance[1:5,])
setwd("C:/Users/nakul/Documents/StudentSuccess")
require(xgboost)
require(Matrix)
require(data.table)
require(caret)
require(mltools)
require(data.table)
require(progress)
require(modeest)
require(xgboost)
require(Matrix)
require(data.table)
require(caret)
require(mltools)
require(data.table)
require(progress)
require(modeest)
df <- read.csv("data/math_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
partition = createDataPartition(
Y,
times = 1,
p = 0.7,
list = TRUE,
groups = min(5, length(Y))
)
X_train = X[partition$Resample1,]
X_test = X[-partition$Resample1,]
Y_train = Y[partition$Resample1]
Y_test = Y[-partition$Resample1]
lm_fit <- glmnet(as.matrix(X_train),as.factor(Y_train),lambda = 0.0447,alpha=1,family = "multinomial")
require(xgboost)
require(Matrix)
require(data.table)
require(caret)
require(mltools)
require(data.table)
require(progress)
require(modeest)
require(glmnet)
df <- read.csv("data/math_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]
Y <- df$Student_Class - 1
X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])
categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)
partition = createDataPartition(
Y,
times = 1,
p = 0.7,
list = TRUE,
groups = min(5, length(Y))
)
X_train = X[partition$Resample1,]
X_test = X[-partition$Resample1,]
Y_train = Y[partition$Resample1]
Y_test = Y[-partition$Resample1]
lm_fit <- glmnet(as.matrix(X_train),as.factor(Y_train),lambda = 0.0447,alpha=1,family = "multinomial")
Y_hat_logistic <- predict(lasso_bestfit,newx = as.matrix(X_test), s=0.0447,type = "class")
lm_fit <- glmnet(as.matrix(X_train),as.factor(Y_train),lambda = 0.0447,alpha=1,family = "multinomial")
Y_hat_logistic <- predict(lm_fit,newx = as.matrix(X_test), s=0.0447,type = "class")
bstDense <- xgboost(data = as.matrix(X_train),
label= Y_train,
objective = "multi:softmax",
num_class = 3,
verbose = 0,
nrounds = 70,
eta = 0.01,
max_depth = 10,
min_child_weight = 1,
gamma = 0.1
)
Y_hat_XGBoost <- as.factor(predict(bstDense, as.matrix(X_test)))
Y_train = factor(Y_train)
train_data <- cbind.data.frame(X_train, Y_train)
Y_test = factor(Y_test)
test_data <- cbind.data.frame(X_test, Y_test)
svm_data <- svm(train_data$Y_train~., data=train_data,kernel = "radial", cross = 5)
require(xgboost)
require(Matrix)
require(data.table)
require(caret)
require(mltools)
require(data.table)
require(progress)
require(modeest)
require(glmnet)
require(e1071)
Y_train = factor(Y_train)
train_data <- cbind.data.frame(X_train, Y_train)
Y_test = factor(Y_test)
test_data <- cbind.data.frame(X_test, Y_test)
svm_data <- svm(train_data$Y_train~., data=train_data,kernel = "radial", cross = 5)
Y_hat_SVM  = as.factor(predict(svm_data, newdata=X_test))
Y_hat_logistic <- as.factor(sample(0:2, length(Y_hat_XGBoost), replace = TRUE))
Y_hat_SVM <- as.factor(sample(0:2, length(Y_hat_XGBoost), replace = TRUE))
predictions <- data.frame(Y_hat_logistic, Y_hat_XGBoost, Y_hat_SVM)
class_chooser <- function(x){
mode <- mlv(x, method = "mfv")
if (length(mode) > 1) {
mode <- sample(mode, 1)
}
mode <- factor(mode, levels = c("0","1","2"))
return(mode)
}
end_predictions = apply(predictions, 1,class_chooser)
c_matrix <- confusionMatrix(data=end_predictions, reference = as.factor(Y_test))
c_matrix
end_predictions
y_test
Y_test
Y_hat_logistic <- as.factor(sample(0:2, length(Y_hat_XGBoost), replace = TRUE))
Y_hat_SVM <- as.factor(sample(0:2, length(Y_hat_XGBoost), replace = TRUE))
predictions <- data.frame(Y_hat_logistic, Y_hat_XGBoost, Y_hat_SVM)
class_chooser <- function(x){
mode <- mlv(x, method = "mfv")
if (length(mode) > 1) {
mode <- sample(mode, 1)
}
mode <- factor(mode, levels = c("0","1","2"))
return(mode)
}
end_predictions = apply(predictions, 1,class_chooser)
predictions$ensemble <- end_predictions
c_matrix <- confusionMatrix(data=end_predictions, reference = as.factor(Y_test))
c_matrix
View(predictions)
Y_hat_logistic <- as.factor(sample(0:2, length(Y_hat_XGBoost), replace = TRUE))
Y_hat_SVM <- as.factor(sample(0:2, length(Y_hat_XGBoost), replace = TRUE))
predictions <- data.frame(Y_hat_logistic, Y_hat_XGBoost, Y_hat_SVM)
class_chooser <- function(x){
mode <- mlv(x, method = "mfv")
if (length(mode) > 1) {
mode <- sample(mode, 1)
}
mode <- factor(mode, levels = c("0","1","2"))
return(mode)
}
end_predictions = apply(predictions, 1,class_chooser)
predictions$ensemble <- end_predictions
predictions$truth <- as.factor(Y_test)
c_matrix <- confusionMatrix(data=end_predictions, reference = as.factor(Y_test))
c_matrix
