---
title: "LogisticRegression"
output: html_notebook
---

## Import Packages

```{r}
require(xgboost)
require(Matrix)
require(data.table)
require(caret)
require(mltools)
require(data.table)
require(progress)
library(glmnet)
library(nnet)
```

# PORTUGESE DATA SET

## Data Prep

### Loading in the data

```{r}
df <- read.csv("data/por_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]

```

### One-Hot Encoding for categorical variables

```{r}
Y <- df$Student_Class - 1

X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])

categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)

```

### Splitting into testing and training sets

```{r}
partition = createDataPartition(
  Y,
  times = 1,
  p = 0.7,
  list = TRUE,
  groups = min(5, length(Y))
)
X_train = X[partition$Resample1,]
X_test = X[-partition$Resample1,]

Y_train = Y[partition$Resample1]
Y_test = Y[-partition$Resample1]


training_set <- cbind(Y_train,X_train)
```

## Generalised Linear Model

## Fit Using Lasso

Finding an optimal lambda to use by using k-fold cross validation.

### K-fold cross-validation

```{r}
cv_fit <- cv.glmnet(as.matrix(X_train),as.factor(Y_train),alpha=1,family = "multinomial")
lasso_fit <- glmnet(as.matrix(X_train),as.factor(Y_train),alpha=1,family = "multinomial")
plot(lasso_fit)
plot(cv_fit)
best_lambda <- cv_fit$lambda.min
best_lambda
```

### Fit-using lasso

```{r}
lasso_bestfit <- glmnet(as.matrix(X_train),as.factor(Y_train),lambda = best_lambda,alpha=1,family = "multinomial")

lasso_coeff <- coef(lasso_bestfit)
lasso_resp <- predict(lasso_bestfit,newx = as.matrix(X_test), 
                       s=best_lambda,type = "class")

c_matrix_lasso <- confusionMatrix(data=as.factor(lasso_resp), reference = as.factor(Y_test))
c_matrix_lasso
```

## Fit Using Ridge Regression

Finding an optimal lambda to use by using k-fold cross validation.

### K-fold cross-validation

```{r}
cv_fit2 <- cv.glmnet(as.matrix(X_train), as.factor(Y_train), alpha = 0,family = "multinomial")
ridge_fit <- glmnet(as.matrix(X_train), as.factor(Y_train), alpha = 0,family = "multinomial")
plot(ridge_fit)
plot(cv_fit2)
best_lambda2 <- cv_fit2$lambda.min
best_lambda2
```

### Fit-using Ridge

```{r}
ridge_bestfit <- glmnet(as.matrix(X_train),as.factor(Y_train),lambda = best_lambda,alpha=0,family = "multinomial")

ridge_coeff <- coef(ridge_bestfit)
ridge_resp <- predict(ridge_bestfit,newx = as.matrix(X_test), 
                       s=best_lambda2,type = "class")
c_matrix_ridge <- confusionMatrix(data=as.factor(ridge_resp), reference = as.factor(Y_test))
c_matrix_ridge
```

# MATH DATASET

## Data Prep

### Loading in the data

```{r}
df <- read.csv("data/math_data_cleaned.csv", header = TRUE)
drops <- c("X","G1","G2","G3", "Student_Score")
df <- df[ , !(names(df) %in% drops)]

```

### One-Hot Encoding for categorical variables

```{r}
Y <- df$Student_Class - 1

X <- df[,names(df) != "Student_Class"]
dtypes <- sapply(X, class)
categorical_names <- names(dtypes[dtypes != "integer"])
numerical_names <- names(dtypes[dtypes == "integer"])

categorical <- X[categorical_names]
numerical <- X[numerical_names]
categorical <- as.data.table(lapply(categorical, as.factor))
categorical <- one_hot(categorical)
X = cbind(categorical, numerical)

```

### Splitting into testing and training sets

```{r}
partition = createDataPartition(
  Y,
  times = 1,
  p = 0.7,
  list = TRUE,
  groups = min(5, length(Y))
)
X_train = X[partition$Resample1,]
X_test = X[-partition$Resample1,]

Y_train = Y[partition$Resample1]
Y_test = Y[-partition$Resample1]


training_set <- cbind(Y_train,X_train)
```

## Generalised Linear Model

```{r eval=FALSE, include=FALSE}
lm_fit <- multinom(Y_train~.,data=training_set,summ = TRUE)
lm_fit
plot(lm_fit)

```

```{r eval=FALSE, include=FALSE}
Y_hat <- predict(lm_fit,newx=as.matrix(X_test),type = 'class' )
c_matrix <- confusionMatrix(data=as.factor(Y_hat), reference = as.factor(Y_test))
c_matrix
```

## Fit Using Lasso

Finding an optimal lambda to use by using k-fold cross validation.

### K-fold cross-validation

```{r}
cv_fit <- cv.glmnet(as.matrix(X_train),as.factor(Y_train),alpha=1,family = "multinomial")
lasso_fit <- glmnet(as.matrix(X_train),as.factor(Y_train),alpha=1,family = "multinomial")
plot(lasso_fit)
plot(cv_fit)
best_lambda <- cv_fit$lambda.min
best_lambda
```

### Fit-using lasso

```{r}
lasso_bestfit <- glmnet(as.matrix(X_train),as.factor(Y_train),lambda = best_lambda,alpha=1,family = "multinomial")

lasso_coeff <- coef(lasso_bestfit)
lasso_resp <- predict(lasso_bestfit,newx = as.matrix(X_test), 
                       s=best_lambda,type = "class")

c_matrix_lasso <- confusionMatrix(data=as.factor(lasso_resp), reference = as.factor(Y_test))
c_matrix_lasso
```

## Fit Using Ridge Regression

Finding an optimal lambda to use by using k-fold cross validation.

### K-fold cross-validation

```{r}
cv_fit2 <- cv.glmnet(as.matrix(X_train), as.factor(Y_train), alpha = 0,family = "multinomial")
ridge_fit <- glmnet(as.matrix(X_train), as.factor(Y_train), alpha = 0,family = "multinomial")
plot(ridge_fit)
plot(cv_fit2)
best_lambda2 <- cv_fit2$lambda.min
best_lambda2
```

### Fit-using Ridge

```{r}
ridge_bestfit <- glmnet(as.matrix(X_train),as.factor(Y_train),lambda = best_lambda,alpha=0,family = "multinomial")

ridge_coeff <- coef(ridge_bestfit)
ridge_resp <- predict(ridge_bestfit,newx = as.matrix(X_test), 
                       s=best_lambda2,type = "class")
c_matrix_ridge <- confusionMatrix(data=as.factor(ridge_resp), reference = as.factor(Y_test))
c_matrix_ridge
```

### 
